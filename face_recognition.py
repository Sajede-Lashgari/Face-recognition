# -*- coding: utf-8 -*-
"""Face Recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iBSK7rGHa32fiEE-6onud0AJPp1vKp3v
"""

# Importing requirement libraries
import numpy as np
import pandas as pd
from sklearn import datasets
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt 
from sklearn.svm import SVC
from datetime import datetime  # For calculating running time algorithm
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report,confusion_matrix 
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import ExtraTreesClassifier
from sklearn import model_selection # random forest model creation
#load data
Olive = datasets.fetch_olivetti_faces()
print(Olive.keys())

#split dataset to train and test data ##25% for testing

x = pd.DataFrame(Olive['data'])
y = pd.DataFrame(Olive['target'])
y = np.ravel(y)

xtrain,xtest, ytrain, ytest = train_test_split(x, y, test_size=0.25, random_state=1)
xtrain

images = Olive.images # save images
images
# note that images can not be saved as features, as we need 2D data for
# features, whereas faces.images are 3D data i.e. (samples, pixel-x, pixel-y)
features = Olive.data  # features
targets = Olive.target # targets

fig = plt.figure() # create a new figure window
for i in range(80): # display 80 images
    # subplot : 10 rows and 10 columns
    img_grid = fig.add_subplot(8, 10, i+1)
    # plot features as image
    img_grid.imshow(images[i])

plt.show()

# Method and algorithm: Random Forest
model = ExtraTreesClassifier() 
model.fit(xtrain,ytrain) 
print(model.feature_importances_)
trees = np.arange(0,40) 
accuracy = np.zeros(40)
Rrf_accuracy = np.zeros(40)
for idx in range(len(trees)):
    classifier = RandomForestClassifier(n_estimators= idx+1, random_state=0)
    classifier.fit(xtrain, ytrain)
    predict = classifier.predict(xtest)
    accuracy[idx] = accuracy_score(ytest, predict) 
    predR = classifier.predict(xtrain)# predict the 'target' for 'training data'
    Rrf_accuracy[idx] = accuracy_score(ytrain, predR)
    
plt.figure(figsize =(15, 10)) 
plt.plot(trees,accuracy ,label = 'Testing Accuracy')
plt.plot(trees, Rrf_accuracy, label = 'Training Accuracy')
plt.legend() 
plt.xlabel('Trees') 
plt.ylabel('Accuracy') 
plt.show()

start = datetime.now()
# Create the model with 13 trees (Selected according to the fracture/elbow plot)
rfc = RandomForestClassifier(n_estimators=13, 
                               bootstrap = True,
                               max_features = 'sqrt',random_state=0)
rfc.fit(xtrain,ytrain)  # predictions
rfc_predS = rfc.predict(xtest)

rf_accuracy = accuracy_score(ytest, rfc_predS)
print("test accuracy:", rf_accuracy)

predR =  rfc.predict(xtrain)# predict the 'target' for 'training data'
Rrf_accuracy = accuracy_score(ytrain, predR)
print("training accuracy:", Rrf_accuracy)

print(classification_report(ytest,rfc_predS, zero_division= 0))   # For testing dataset (You can also use it for training dataset)
print(confusion_matrix(ytest,rfc_predS))
print(rfc_predS)
def loop():
    k = 0
    for i in range(100):
        k+=i
    print(k)
loop()
print(datetime.now()-start)

# Method and algorithm: KNN 
neighbors = np.arange(1, 30,2) 
train_accuracy = np.empty(len(neighbors)) 
test_accuracy = np.empty(len(neighbors)) 
test_error = np.empty(len(neighbors))
train_error = np.empty(len(neighbors)) 

# Loop over K values 
for i, k in enumerate(neighbors): 
	knn = KNeighborsClassifier(n_neighbors=k) 
	knn.fit(xtrain, ytrain) 
	
	# Compute traning and test data accuracy 
	train_accuracy[i] = knn.score(xtrain, ytrain) 
	test_accuracy[i] = knn.score(xtest, ytest)
	test_error[i] = 1 - test_accuracy[i]
	train_error[i] = 1 - train_accuracy[i]
    
# Generate plot 
plt.figure(figsize =(15, 10)) 
plt.plot(neighbors, test_error, label = 'Testing Error') 
plt.plot(neighbors, train_error, label = 'Training Error') 

plt.legend() 
plt.xlabel('k') 
plt.ylabel('Error') 
plt.show()

plt.figure(figsize =(15, 10)) 
plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy') 
plt.plot(neighbors, train_accuracy, label = 'Training Accuracy') 

plt.legend() 
plt.xlabel('k') 
plt.ylabel('Accuracy') 
plt.show()

start = datetime.now()
knn = KNeighborsClassifier(n_neighbors=3)  # Selected according to the fracture/elbow plot
knn.fit(xtrain,ytrain)
predT = knn.predict(xtest)

# Compute traning and test data accuracy 
train_accuracy = knn.score(xtrain, ytrain) 
test_accuracy = knn.score(xtest, ytest)
print("train accuracy:",train_accuracy) #training accuracy
print("test accuracy:",test_accuracy) #testing accuracy

predN = knn.predict(xtrain) #predict for training set
print(confusion_matrix(ytest,predT)) 
print(classification_report(ytest,predT, zero_division= 0))
print(predT)
def loop():
    k = 0
    for i in range(100):
        k+=i
    print(k)
loop()
print(datetime.now()-start)

# Method and algorithm: Support Vector Machine with Linear kernel
c = np.arange(0.1, 4, 0.1)
svmtest_accuracy = np.zeros(39)
svmtrain_accuracy = np.zeros(39)
for idx in np.nditer(c):
    svclassifier = SVC(kernel='linear', C=idx)
    svclassifier.fit(xtrain, ytrain)
    prediction_test = svclassifier.predict(xtest)
    svmtest_accuracy[int(idx*10)-1] = accuracy_score(ytest, prediction_test)
    prediction_train = svclassifier.predict(xtrain)
    svmtrain_accuracy[int(idx*10)-1] = accuracy_score(ytrain, prediction_train)

plt.figure(figsize =(15, 10)) 
plt.plot(c, svmtrain_accuracy, label = 'Training Accuracy')
plt.plot(c, svmtest_accuracy, label = 'Testing Accuracy')
plt.legend() 
plt.xlabel('C')
plt.ylabel('Accuracy') 
plt.show()

print("train accuracy:", svmtrain_accuracy) #training accuracy
print("test accuracy:", svmtest_accuracy) #testing accuracy

# Method and algorithm: Support Vector Machine with rbf kernel
c = np.arange(0.1, 4, 0.1)
svmtest_accuracy = np.zeros(39)
svmtrain_accuracy = np.zeros(39)
for idx in np.nditer(c):
    svclassifier = SVC(kernel='rbf', C=idx)
    svclassifier.fit(xtrain, ytrain)
    prediction_test = svclassifier.predict(xtest)
    svmtest_accuracy[int(idx*10)-1] = accuracy_score(ytest, prediction_test)
    prediction_train = svclassifier.predict(xtrain)
    svmtrain_accuracy[int(idx*10)-1] = accuracy_score(ytrain, prediction_train)
  
plt.figure(figsize =(15, 10)) 
plt.plot(c, svmtrain_accuracy, label = 'Training Accuracy')
plt.plot(c, svmtest_accuracy, label = 'Testing Accuracy')
svmtest_accuracy
plt.legend() 
plt.xlabel('C')
plt.ylabel('Accuracy') 
plt.show()

print("train accuracy:", svmtrain_accuracy) #training accuracy
print("test accuracy:", svmtest_accuracy) #testing accuracy

# Method and algorithm: Support Vector Machine with poly kernel
c = np.arange(0.1, 4, 0.1)
svmtest_accuracy = np.zeros(39)
svmtrain_accuracy = np.zeros(39)
for idx in np.nditer(c):
    svclassifier = SVC(kernel='poly', C=idx)
    svclassifier.fit(xtrain, ytrain)
    prediction_test = svclassifier.predict(xtest)
    svmtest_accuracy[int(idx*10)-1] = accuracy_score(ytest, prediction_test)
    prediction_train = svclassifier.predict(xtrain)
    svmtrain_accuracy[int(idx*10)-1] = accuracy_score(ytrain, prediction_train)

plt.figure(figsize =(15, 10))
plt.plot(c, svmtrain_accuracy, label = 'Training Accuracy')
plt.plot(c, svmtest_accuracy, label = 'Testing Accuracy')
svmtest_accuracy
plt.legend() 
plt.xlabel('C')
plt.ylabel('Accuracy') 
plt.show()

print("train accuracy:", svmtrain_accuracy) #training accuracy
print("test accuracy:", svmtest_accuracy) #testing accuracy

start = datetime.now()
svclassifier = SVC(kernel='linear' , C=.1)
svclassifier.fit(xtrain, ytrain)
ypred = svclassifier.predict(xtest) # predict the 'target' for 'testing data'
svm_accuracy = accuracy_score(ytest, ypred)
print("testing accuracy:", svm_accuracy)
prediction_train = svclassifier.predict(xtrain) # predict the 'target' for 'training data'
svmtrain_accuracy = accuracy_score(ytrain, prediction_train)

print("training accuracy:", svmtrain_accuracy)
print(confusion_matrix(ytest, ypred))
print(classification_report(ytest, ypred, zero_division= 0 ))
print(ypred)
def loop():
    k=0
    for i in range(100):
        k+=i
    print(k)
loop()
print(datetime.now()-start)

# Step2: dimensionality reduction with PCA

pca = PCA(.95)  #with 95% variance 
principalComponents = pca.fit_transform(x)
principalDf = pd.DataFrame(data = principalComponents)
nxtrain,nxtest, nytrain, nytest = train_test_split(principalDf, y, test_size=0.25, random_state=1)

principalDf

#Using less dimensional data

model = ExtraTreesClassifier() 
model.fit(nxtrain,nytrain) 
print(model.feature_importances_)
trees=np.arange(0,40) 
accuracy=np.zeros(40)
Rrf_accuracy=np.zeros(40)
for idx in range(len(trees)):
    classifier = RandomForestClassifier(n_estimators=idx+1)
    classifier.fit(nxtrain,nytrain)
    predict=classifier.predict(nxtest)
    accuracy[idx] = accuracy_score(nytest,predict) 
    predR =  classifier.predict(nxtrain)# predict the 'target' for 'training data'
    Rrf_accuracy[idx] = accuracy_score(nytrain, predR)

plt.figure(figsize =(15, 10)) 
plt.plot(trees,accuracy ,label = 'Testing Accuracy')
plt.plot(trees, Rrf_accuracy, label = 'Training Accuracy')
plt.legend() 
plt.xlabel('Trees') 
plt.ylabel('Accuracy') 
plt.show()

start = datetime.now()
# Create the model with 13 trees
rfc = RandomForestClassifier(n_estimators=13, 
                               bootstrap = True,
                               max_features = 'sqrt',random_state=0)
rfc.fit(nxtrain,nytrain)  # predictions
rfc_predS = rfc.predict(nxtest)

rf_accuracy = accuracy_score(nytest, rfc_predS)
print("test accuracy:", rf_accuracy)
predR = rfc.predict(nxtrain)# predict the 'target' for 'training data'
Rrf_accuracy = accuracy_score(nytrain, predR)
print("training accuracy:", Rrf_accuracy)
print(confusion_matrix(ytest,rfc_predS))
print(classification_report(ytest,rfc_predS, zero_division=0))
print(rfc_predS)
def loop():
    k = 0
    for i in range(100):
        k+=i
    print(k)
loop()
print(datetime.now()-start)

neighbors = np.arange(1, 30,2) 
train_accuracy = np.empty(len(neighbors)) 
test_accuracy = np.empty(len(neighbors)) 
test_error = np.empty(len(neighbors))
train_error = np.empty(len(neighbors)) 

# Loop over K values 
for i, k in enumerate(neighbors): 
	knn = KNeighborsClassifier(n_neighbors=k) 
	knn.fit(nxtrain, nytrain) 
	
	# Compute traning and test data accuracy 
	train_accuracy[i] = knn.score(nxtrain, nytrain) 
	test_accuracy[i] = knn.score(nxtest, nytest)
	test_error[i] = 1 - test_accuracy[i]
	train_error[i] = 1 - train_accuracy[i]

plt.figure(figsize =(15, 10)) 
plt.plot(neighbors, test_error, label = 'Testing Error') 
plt.plot(neighbors, train_error, label = 'Training Error') 
plt.legend() 
plt.xlabel('k') 
plt.ylabel('Error') 
plt.show()

plt.figure(figsize =(15, 10)) 
plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy') 
plt.plot(neighbors, train_accuracy, label = 'Training Accuracy') 
plt.legend() 
plt.xlabel('k') 
plt.ylabel('Accuracy') 
plt.show()

start = datetime.now()
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(nxtrain,nytrain)
predT = knn.predict(nxtest)
# Compute traning and test data accuracy 
train_accuracy = knn.score(nxtrain, nytrain) 
test_accuracy = knn.score(nxtest, nytest)

print("train accuracy:",train_accuracy)
print("test accuracy:",test_accuracy)
print(confusion_matrix(nytest,predT))
print(classification_report(nytest,predT, zero_division = 0))
print(predT)
def loop():
    k = 0
    for i in range(100):
        k+=i
    print(k)
loop()
print(datetime.now()-start)

c = np.arange(0.1,4,0.1)
svmtest_accuracy = np.zeros(39)
svmtrain_accuracy = np.zeros(39)
for idx in np.nditer(c):
    svclassifier = SVC(kernel='poly', C=idx)
    svclassifier.fit(nxtrain, nytrain)
    prediction_test = svclassifier.predict(nxtest)
    svmtest_accuracy[int(idx*10)-1] = accuracy_score(nytest, prediction_test)
    prediction_train = svclassifier.predict(nxtrain)
    svmtrain_accuracy[int(idx*10)-1] = accuracy_score(ytrain, prediction_train)

plt.figure(figsize =(15, 10)) 
plt.plot(c, svmtrain_accuracy, label = 'Training Accuracy')
plt.plot(c, svmtest_accuracy, label = 'Testing Accuracy')
plt.legend() 
plt.xlabel('C')
plt.ylabel('Accuracy') 
plt.show()

print("train accuracy:", svmtrain_accuracy)
print("test accuracy:", svmtest_accuracy)

c = np.arange(0.1,4,0.1)
svmtest_accuracy = np.zeros(39)
svmtrain_accuracy = np.zeros(39)
for idx in np.nditer(c):
    svclassifier = SVC(kernel='rbf', C=idx)
    svclassifier.fit(nxtrain, nytrain)
    prediction_test = svclassifier.predict(nxtest)
    svmtest_accuracy[int(idx*10)-1] = accuracy_score(nytest, prediction_test)
    prediction_train = svclassifier.predict(nxtrain)
    svmtrain_accuracy[int(idx*10)-1] = accuracy_score(ytrain, prediction_train)

plt.figure(figsize =(15, 10)) 
plt.plot(c, svmtrain_accuracy, label = 'Training Accuracy')
plt.plot(c, svmtest_accuracy, label = 'Testing Accuracy')
plt.legend() 
plt.xlabel('C')
plt.ylabel('Accuracy') 
plt.show()

print("train accuracy:", svmtrain_accuracy)
print("test accuracy:", svmtest_accuracy)

c = np.arange(0.1,4,0.1)
svmtest_accuracy = np.zeros(39)
svmtrain_accuracy = np.zeros(39)
for idx in np.nditer(c):
    svclassifier = SVC(kernel='linear', C=idx)
    svclassifier.fit(nxtrain, nytrain)
    prediction_test = svclassifier.predict(nxtest)
    svmtest_accuracy[int(idx*10)-1] = accuracy_score(nytest, prediction_test)
    prediction_train = svclassifier.predict(nxtrain)
    svmtrain_accuracy[int(idx*10)-1] = accuracy_score(ytrain, prediction_train)

plt.figure(figsize =(15, 10)) 
plt.plot(c, svmtrain_accuracy, label = 'Training Accuracy')
plt.plot(c, svmtest_accuracy, label = 'Testing Accuracy')
plt.legend() 
plt.xlabel('C')
plt.ylabel('Accuracy') 
plt.show()

print("train accuracy:", svmtrain_accuracy)
print("test accuracy:", svmtest_accuracy)

start = datetime.now()
svclassifier = SVC(kernel='linear', C=.1)
svclassifier.fit(nxtrain, nytrain)
ypred = svclassifier.predict(nxtest) # predict the 'target' for 'testing data'
svm_accuracy = accuracy_score(nytest, ypred)
print("testing accuracy:", svm_accuracy)
prediction_train = svclassifier.predict(nxtrain) # predict the 'target' for 'training data'
svmtrain_accuracy = accuracy_score(nytrain, prediction_train)

print("training accuracy:", svmtrain_accuracy)
print(confusion_matrix(nytest,ypred))
print(classification_report(nytest, ypred, zero_division=0))
print(ypred)
def loop():
    k = 0
    for i in range(100):
        k+=i
    print(k)
loop()
print(datetime.now()-start)

start = datetime.now()
svclassifier = SVC(kernel='rbf', C=.6)
svclassifier.fit(nxtrain, nytrain)
ypred = svclassifier.predict(nxtest) # predict the 'target' for 'testing data'
svm_accuracy = accuracy_score(nytest, ypred)
print("testing accuracy:", svm_accuracy)
prediction_train = svclassifier.predict(nxtrain) # predict the 'target' for 'training data'
svmtrain_accuracy = accuracy_score(nytrain, prediction_train)
print("training accuracy:", svmtrain_accuracy)
print(confusion_matrix(nytest,ypred))
print(classification_report(nytest ,ypred, zero_division=0))
print(ypred)
def loop():
    k = 0
    for i in range(100):
        k+=i
    print(k)
loop()
print(datetime.now()-start)

start = datetime.now()
svclassifier = SVC(kernel='poly', C=.4)
svclassifier.fit(nxtrain, nytrain)
ypred = svclassifier.predict(nxtest) # predict the 'target' for 'testing data'
svm_accuracy = accuracy_score(nytest, ypred)
print("testing accuracy:", svm_accuracy)
prediction_train = svclassifier.predict(nxtrain) # predict the 'target' for 'training data'
svmtrain_accuracy = accuracy_score(nytrain, prediction_train)
print("training accuracy:", svmtrain_accuracy)
print(confusion_matrix(nytest ,ypred))
print(classification_report(nytest ,ypred, zero_division=0))
print(ypred)
def loop():
    k = 0
    for i in range(100):
        k+=i
    print(k)
loop()
print(datetime.now()-start)

from sklearn.model_selection import cross_val_score
accuracy = cross_val_score(RandomForestClassifier(n_estimators=13, 
                               bootstrap = True,
                               max_features = 'sqrt',random_state=0), x, y, scoring='accuracy', cv = 10)
print(accuracy)
#get the mean of each fold 
print("Accuracy of Model with Cross Validation is:",accuracy.mean() * 100)

accuracy = cross_val_score(SVC(kernel='linear', C=.1), x, y, scoring='accuracy', cv = 10)
print(accuracy)
#get the mean of each fold 
print("Accuracy of Model with Cross Validation is:",accuracy.mean() * 100)

accuracy = cross_val_score(KNeighborsClassifier(n_neighbors=3), x, y, scoring='accuracy', cv = 10)
print(accuracy)
#get the mean of each fold 
print("Accuracy of Model with Cross Validation is:",accuracy.mean() * 100)

